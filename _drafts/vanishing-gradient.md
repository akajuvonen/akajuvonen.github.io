---
layout: post
title:  "Vanishing Gradient Problem in Deep Neural Nets"
---

Some neural network basics. Feed-forward. Backpropagation learning.

Many more layers = deep learning. Now this problem arises.

Example of vanishing gradient. Use sigmoid function as an example.
This is why sigmoid is dead! Sigmoid derivative max 0.25.

Maybe mention exploding gradient, as well?

How to combat this? Relu of some other activation function?
